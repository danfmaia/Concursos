# BACEN - Disciplina 6 - Ciência de Dados

Prof. Vitor Kessler

- @vitor_kessler

## Edital

1. **Aprendizado de Máquina**.

2. Deep Learning.

3. Processamento de Linguagem Natural (PLN).

4. Big data.

5. Qualidade de dados.

6. **Tipos de Aprendizado:**

   1. **Supervisionado**,
   2. **Não Supervisionado**,
   3. **Semi Supervisionado**,
   4. **Por Reforço**,
   5. **Por Transferência**.

7. Grandes Modelos de Linguagem (LLM), IA Generativa

8. **Redes Neurais**.

9. MLOps:

   1. Gestão de código,
   2. Treinamento,
   3. Implantação,
   4. Monitoramento e Versionamento de modelos,
   5. Automação do ciclo de produção.

10. Governança e Ética na IA:

    1. Transparência,
    2. Responsabilidade,
    3. Explicabilidade,
    4. Privacidade,
    5. Segurança,
    6. Viés.

# 6.1 - Inteligência Artificial (IA)

## Agenda do Curso

1. Introdução à IA

2. Aprendizado de Máquina (AM) - Introdução

3. Classificação

   - Classificam exemplos em classes.

4. Redes Neurais Artificiais (RNA)

5. Regressão

   - Fazem previsões em um continuum.
   - É uma espécie de Classificação.

6. Clusterização (Agrupamento)

## 6.1.1 - Introdução à IA

- Campo _multidisciplinar_ da ciência da computação.

- Desenvolve _tecnologias autônomas_ p/ executar funções.

- Desenvolvimento de distemas e algoritmos capazes de realizar tarefas que normalmente exigem _inteligência humana_:

  - Criatividade
  - Auto aperfeiçoamento
  - Uso da linguagem
  - Visão computacional

- Os programas...

  - Aprendem.
  - Raciocionam.
  - Reconhecem padrões.
  - Inferem.
  - Tomam decisões.
  - Solucionam problemas.
  - Processam e analisam Big Data—grandes volumes de dados—de forma rápida e eficiente.

### Abordagens principais

- IA Simbólica:

  - Representação de conhecimento por meio de símbolos lógicos.
  - Estopim:
    > Sistemas especialistas baseados em lógica de primeira ordem construído com o Prolog.
  - Conhecimento programado diretamente por humanos.

- IA Conexionista:

  - Baseada em _redes neurais artificiais_.
  - Uso de _Machine Learning_.
  - Necessidade de _Big Data_.

### Tipos de IA

- Tipos segundo a Capacidade:

  - IA limitada (ou estreita).
  - IA geral.
  - Super IA.

- Tipos segundo a Funcionalidade:

  - Máquina reativa.
  - Memória limitada.
  - Teoria da mente.
  - Autoconsciente.

### Desafios da IA

- Processamento insuficiente de dados.
- Capacidade limitada de hardware.
- Interface Homem-Máquina.
- Custos altos.
- Considerações éticas.

### Áreas da IA

- Aprendizado de Máquina (AM) (Machine Learning) (ML)
  > Algoritmos que permitem que as máquinas aprendam com os dados e melhorem seu desempenho ao longo do tempo.
- Redes Neurais Artificiais
  > Modelos inspirados no funcionamento do cérebro humano que são usados p/ processar informações e tomar decisões.
- Processamento de Linguagem Natural (PLN)
  > O estudo da interação entre máquinas e linguagem humana, incluindo tarefas como tradução automática, reconhecimento de fala e análise de sentimento.
- Visão Computacional
  > Desenvolvimento de algoritmos e de sistemas capazes de entender e de interpretar imagens e vídeos.
- Robótica Inteligente
  > Combinação de IA e robótica p/ criar robôs capazes de interagir com o ambiente e realizar tarefas complexas.
- Sistemas Especialistas
  > Sistemas de IA projetados p/ imitar a inteligência e o conhecimento de especialistas humanos em um domínio específico.
- Algoritmos Genéticos
  > Técnicas de otimização baseadas em princípios genéticos e em evolução natural, usadas p/ resolver problemas complexos.
- Sistemas de Recomendação
  > Algoritmos que analisam dados e padrões de comportamento do usuário p/ fornecer sugestões personalizadas.
- IA Conversacional
  > Tecnologias que permitem a interação entre humanos e sistemas de IA por meio de diálogos naturais, como chatbots e assistentes virtuais.
- Mineração de Dados (Data Mining)
  > Técnicas p/ descobrir padrões e conhecimentos úteis a partir de conjuntos de dados grandes e complexos.
- Reconhecimento de Padrões
  > Identificação de padrões em dados, como reconhecimento facial, detecção de objetos e análise de imagem.
- IA Explicável
  > Desenvolvimento de métodos e de algoritmos que permitem entender e explicar o processo de tomada de decisão de sistemas de IA.
- Agentes Inteligentes
  > Programas de computador capazes de tomar decisões e agir de forma autônoma em um ambiente específico.
- Aprendizado Profundo (Deep Learning)
  > Subcampo do Machine Learning, utiliza Redes Neurais (unidades conectadas em rede p/ a análise de bancos de dados e informações) p/ emular o cérebro humano.
- Processamento de Sinais
  > Análise e interpretação de sinais (como áudio e vídeo) utilizando técnicas de IA.
- IA Responsável
  > Envolve garantir que a IA seja desevolvida e implementada de forma ética e legalmente responsável.
- Raciocínio Automatizado (Automated Reasoning)
  > Desenvolvimento de algoritmos e de técnicas p/ automatizar o processo de raciocínio lógico, permitindo que os computadores cheguem a conclusões ou provem teoremas de forma automatizada.

### 6.1.2 - Aprendizado de Máquina (AM)

- Def. — É a área da IA cujo objetivo é o _desenvolvimento de técnicas computacionais sobre o aprendizado_ bem como a construção de sistemas capazes de _adquirir conhecimento de forma automática_.

- [ _Vide figura do slide._ ]

### Hierarquia do conhecimento

1. Dado
2. Informação
3. Conhecimento
4. Sabedoria

### Definições de AM

- "Aprender implica em alterações no sistema que são adaptativas, no sentido que elas capacitam o sistema a realizar a mesma tarefa, ou tarefas provenientes da mesma população, de forma mais eficiente e eficaz na próxima vez." (Simon, 1983)

- "Um sistema de aprendizado [supervisionado] é um programa de computador que toma decisões baseadas na experiência contida em exemplos solucionados com sucesso." (Weiss & Kulikowski, 1991)

- "...todo aprendizado pode ser visto como o aprendizado de uma função" (Russel & Norvig, 1995)

### Tarefas de AM

- Tarefas **Descritivas**

  > Def. — Busca-se desenvolver algoritmos que _descreverão os dados_.

  > E.g: Agrupamento.

- Tarefas **Preditivas**

  > Def. — Busca-se desenvolver algoritmos p/ _realizar previsões de algo a partir de uma entrada de dados_.

  > Podem ser divididas em tarefas de _Classificação_ e de _Regressão_.

### Paradigmas de AM

- **Simbólico**

  > Def. — Construção de uma _representação simbólica de um conceito por meio de exemplos e contra-exemplos_.

  > Representação simbólica _na forma de alguma expressão lógica_, como árvores de decisão e regras.

- **Protótipo** ou **Memorização** (Instance Based)

  > Sistema que classifica um exemplo por meio de _exemplos similares conhecidos_.

  > Ex.: K-NN.

  - Sub-paradigmas:

    - Sistemas preguiçosos (lazy)
      > Necessitam manter os exemplos na memória p/ classificar novos exemplos.
    - Sistemas gulosos (eager)
      > Utilizam os exemplos p/ induzir o modelo, descartando-os logo após.

- **Conexionista**

  > Def. — Envolvem _unidades altamente interconectadas_.

  > Único ex. relevante: _Redes neurais_.

- **Genético**

  > Def. — Um classificador genético consiste de uma _população de elementos de classificação que competem p/ fazer a predição_.

  > Elementos que possuem um desempenho ruim são descartados, enquanto os elementos mais fortes proliferam, _produzindo variações_.

  > Alguns operadores genéticos básicos que aplicados a população geram novos indivíduos são: _Reprodução, Cruzamento, Mutação e Inversão_.

- **Estatístico**

  > Def. — Utilização de _modelos estatísticos p/ encontrar uma boa aproximação do conceito induzido_.

  > Vários desses métodos são _paramétricos_, assumindo alguma forma de modelo, e então encontrando valores apropriados p/ os parâmetros do modelo a partir dos exemplos.

  > Dentre os métodos estatísticos, destacam-se os de _aprendizado Bayesiano_, que utilizam um modelo probabilístico baseado no conhecimento prévio do problema, o qual é combinado com os exemplos de treinamento p/ determinar a probabilidade final de uma hipótese. Ex.: Naive Bayes.

### Árvore do Aprendizado Indutivo

- Aprendizado Indutivo

  - Aprendizado Supervisionado

    - Classificação

    - Regressão

  - Aprendizado Não Supervisionado

### Tipos de sistema de aprendizado

- **Não Simbólico** ou **Caixa-preta**

  - Não facilmente interpretado por humanos.

  - Desenvolve sua própria representação de conceitos.

  - Não fornece esclarecimento ou explicação sobre o processo de classificação.

- **Simbólico** ou **Orientado a Conhecimento**

  - Cria estruturas simbólicas que podem ser mais facilmente compreendidas por seres humanos.

  - "Os resultados da indução devem ser descrições simbólicas das entidades dadas... devem ser compreensíveis como simples 'pedados' de informação, diretamente interpretáveis em linguagem natural..." (Michalski 1983a)

### Outros conceitos importantes

- **Indutor**

  > Def. — Programa que gera uma hipótese (classificador) a partir de um conjunto de exemplos.

- **Exemplo, Caso ou Registro** (Instance)

  > Def. — É um conjunto fixo de atributos.

  > Um exemplo descreve o objeto de interesse, tal como um paciente, exemplos médicos sobre uma determinada doença, ou histórico de clientes de uma dada companhia.

- **Atributo ou Campo** (Feature)

  > Def. — Uma única característica de um exemplo.

- **Domínio**

  > Def. — Conjunto de valores que um atributo pode assumir.

- **Classe**

  > Def. — Atributo especial que descreve o fenômeno de interesse.

  > Aplicável somente no _Aprendizado Supervisionado_.

### Tipos de Atributo

- **Nominal** (ou Discreto ou Categórico)

  > Def. — O atributo assume valores em um conjunto finito.

  > Alguns indutores podem aceitar uma subdivisão entre os atributos nominais:

  - Ordenado
    > O domínio é ordenado, mas a diferença absoluta dos valores é desconhecida. E.g. escala de temperatura: baixa, média ou alta; severidade de um machucado.
  - Não-ordenado

    > Não existe uma ordem entre os valores. E.g. cor: vermelho, verde, azul; ocupação; estado civil; raça.

- **Contínuo** (ou Numérico ou Real)

  > Def. — O domínio do atributo é ordenado e pode ser representado por um valor real (e.g., peso ∈)

- **Desconhecido ou Faltante**

- **Não se aplica**

- **Relevante e Irrelevante**

  > Um atributo é irrelevante se existe uma descrição completa e consistente das classes a serem aprendidas que não usa aquele atributo.

> A busca é por atributos _com alto poder preditivo_.

### Conjunto de Treinamento e de Testes

- [ _Vide slides._ ]

### Classificador

- Def. — Dado um conjunto de treinamento, _um indutor gera como saída um classificador_ (hipótese ou descrição de conceito) de forma que, dado um novo exemplo, ele possa _predizer precisamente sua classe_.

- Cada exemplo é um par (x, f(x)), onde x é a entrada e f(x) é a saída (f desconhecida!).

- y = f(x) assume valores discretos y ∈ {C1, C2, ..., Ck} → _Classificação_.

- y = f(x) assume valores reais → _Regressão_.

- **Indução ou inferência indutiva**

  > Ex.: Dada uma coleção de exemplos de f(x), retornar uma função h(x) que aproxima f(x). Ou seja, h(x) ≅ f(x).

- **Bias** (viés)

  > Def. — Qualquer critério de preferência de uma hipótese sobre outra (além da consistência com os exemplos).

- **Indutor Instável vs. Estável**

  - Instável
    > Pequenas pertubação (variação) no conjunto de treinamento pode causar modificação no classificador gerado.
  - Estável
    > O classificador gerado não muda muito quando os exemplos de treinamento se alteram.

- **Trade-off entre Bias e Variância**

  - Indutores Instáveis
    > Em geral, geram classificadores com _alta variância_ mas com _pequeno bias_.
  - Indutores Estáveis
    > Em geral, geram classificadores com _baixa variância_ mas com _alto bias_.

- Indutor **Não Incremental** (modo batch) vs. Indutor **Incremental**

### Overfitting vs. Undertraining

> [ _Vide slide com gráfico._ ]

- Overfitting (overtraining)

  > Def. — A hipótese extraída a partir dos exemplos é muito específica p/ o conjunto de treinamento.

  - Para se **combater o overfitting**, pode-se:

    - > Diminuir a complexidade do modelo, e
    - > Usar métodos de validação cruzada.

- Underfitting

  > Def. — A hipótese induzida apresenta um desempenho ruim tanto no conjunto de treinamento como de teste.

  - Casos comuns:

    - > Poucos exemplos representativos foram dados ao sistema de aprendizado (e.g. algoritmos de árvore de decisão ou de indução de regras).

    - > O usuário pré-definiu um tamanho muito pequeno p/ o classificador (e.g. insuficientes neurônios em uma rede neural ou um alto valor de poda p/ árvores de decisão).

### Consistência e Completude de um classificador

- Completude

  > Tem a ver com o grau de preenchimento do espaço de exemplos do problema.

- Consistência

  > Tem a ver com a _taxa de acerto_ do classificador.

### Aprendizado Supervisionado vs. Não Supervisionado

- Aprendizado **Supervisionado**

  - Compreender o relacionamento entre os atributos e a classe.
  - Predizer a classe de novos exemplos o melhor possível.

- Aprendizado **Não Supervisionado**

  - Encontrar representações úteis dos exemplos, tais como:
    - Encontrar agrupamentos (clusters),
    - Redução da dimensão,
    - Encontrar as causas ou as fontes ocultas dos exemplos,
    - Modelar a densidade dos exemplos.

## 6.1.2 - Métricas de Classificação

### Matriz de Confusão

- > Def. — É uma tabela que permite a visualização do desempenho de um algoritmo de classificação.

- > [ Vide SLIDE p/ matriz. ]

```
                             Valor Previsto
                         Irregular | Ñ-Irregular
Valor      | Irregular       VP          FN
Verdadeiro | Ñ-Irregular     FP          VN
```

### Métricas

- **Positivos e Negativos:**

  - Verdadeiro Positivo (VP)
  - Verdadeiro Negativo (VN)
  - Falso Positivo (FP)
    > Erro do tipo I
  - Falso Negativo (FN)
    > Erro do tipo II

- **Acurácia e Erro:**

  > A Acurácia mede o percentual geral de acertos.

  - acurácia = (VP + VN) / total de amostras

  - erro = 1 - acurácia

- **Precisão:**

  > Indica, p/ as classificações _positivas_ do modelo, quantas foram acertadas.

  - precisão = VP / (VP + FP)

- **Recall ou Revocação ou Sensibilidade:**

  > Indica, p/ as amostras positivas existentes, quantas o modelo classificou corretamente.

  - recall = VP / (VP + FN)

- **Especificidade:**

  > Avalia a capacidade do método de detectar resultados negativos.

  - especificidade = VN / (VN + FP)

- F-Measure, F-Score ou Score F1:

  - f1 = 2 \* (precisão \* recall) / (precisão + recall)

### Resumo de Fórmulas

- **acurácia = (VP + VN) / total de amostras**
- erro = 1 - acurácia
- **precisão = VP / (VP + FP)**
- **recall = VP / (VP + FN)**
- especificidade = VN / (VN + FP)

### Navalha de Occam

- > Do latim "lex parsimoniae" — Lei da Parcimônia (conceito da economia).

- > Def. — A explicação de qualquer fenômeno deve fazer o _menor número possível de suposições_, eliminando aquelas que não fazem diferença nas predições observáveis da hipótese explicativa ou da teoria.

- **Dilema de Occam:**

  > Em AM, Acurácia e Simplicidade estão em conflito.

### Maldição da Dimensionalidade

- > Aprender a partir de um espaço de característica de _alta dimensionalidade_ requer uma _quantidade enorme de dados de treinamento_ p/ garantir que haja várias amostras com cada combinação de valores.

- > Def. — Com uma _quantidade fixa_ de número de instâncias de treinamento, _o poder de preditibilidade REDUZ à medida que aumenta a dimensionalidade_.

## 6.1.3 - Introdução à Classificação

- > Um problema de Classificação tem um _valor discreto_ como saída.

- > Def. — Classificação tem como finalidade atribuir um objeto/evento a uma categoria, pertencente a um conjunto finito de categorias.

### Aplicações

- Diagnóstico médico
- Detecção de fraude em cartões de crédito
- Detecção de vírus em redes de computadores
- Filtragem de spam em e-mails
- Bioinformática (sequências de DNA)
- Reconhecimento de caracteres
- Reconhecimento de imagens

### Exemplos de Classificador

- Árvores de Decisão
- Random Forest
- k-Nearest Neighbors (k-NN)
- Naive Bayes
- Regressão Logística
- Análise Discriminante Linear
- SVM (Support Vector Machine)
- **RNA (Redes Neurais Artificiais)**

## 6.1.4 - Árvore de Decisão

- > Def. — É uma técnica utilizada p/ Classificação e _consiste em um mapa dos possíveis resultados de uma série de escolhas_ (White Model).

- > É formada por _3 partes principais_:

  - **Raiz**
    > É a primeira decisão a ser tomada pelo usuário.
  - **Nós**
    > São todas as decisões apresentadas na árvore.
  - **Folhas**
    > São os resultados da árvore de deciões. É nela que a Classificação é realizada.

  E.g. [ Vide SLIDES ]

- > Podem ser reescritas como um _conjunto de regras_, e.g. em Forma Normal Disjuntiva (DNF).

  - E.g. [ Vide SLIDE ]

- > Atributos contínuos podem ser usados, fazendo o nó dividir o domínio do atributo entre 2 intervalos baseados em um limite.

  - E.g. tamanho < 3 e tamanho >= 3.

- > Árvores de Classificação têm valores _discretos_ nas folhas. Árvores de Regressão têm valores _reais_ nas folhas.

- > Algoritmos p/ encontrar árvores consistentes são _eficientes_ e podem processar _grandes quantidades de dados de treinamento_.

- > _Não necessita de manipulação de dados_, como por ex. métodos de Normalização.

- > Alguns conceitos são de _difícil aprendizagem em Árvores de Decisão_, gerando Árvores extremamente grandes. E.g. XOR.

- > A maioria dos algoritmos de aprendizagem de Árvores _derivam do algoritmo ID3_:

  - > C4.5 e C5.0 são os mais recentes.
  - > O ID3 aprende a Árvore usando uma estratégia Top-Down.

### Algoritmo básico

- > O melhor atributo é selecionado e usado como Raiz da Árvore.

- > Um descendente (Sub-Árvore) do Nó raiz é então criado p/ cada valor possível deste atributo, e os exemplos de treinamento são ordenados p/ o Nó descendente apropriado.

- > O processo é repetido usando exemplos com cada Nó descendente p/ selecionar o melhor atributo p/ avaliar naquele ponto da Árvore.

### Overfitting

- [ Vide SLIDE p/ gráfico. ]

- P/ evitar Overfitting:

  - **Pré-Poda**

    > _Parar de crescer a Árvore_ quando não há mais dados suficientes p/ fazer previsões confiáveis.

  - **Pós-Poda**

    > _Construir a Árvore toda_, depois _remover Sub-Árvores com menos relevância_.

  - Métodos p/ Poda:

    - Validação Cruzada
      > Reservar alguns dados de treinamento (Conjunto de Validação) p/ avaliar utilidade das Sub-Árvores.
    - Testes Estatísticos
      > Usar o teste p/ determinar se a regularidade observada foi devida ao acaso.
    - Comprimento da Descrição Mínima (MDL)
      > Determinar se a complexidade adicional da hipótese é mais complexa que lembrar explicitamente as exceções resultantes da poda.

### Entropia de uma Árvore de Decisão

- > Def. — Aborda o aspecto da _quantidade de informações associadas às respostas que podem ser obtidas às perguntas formuladas_, representando o **Grau de Incerteza** associado aos dados.

- > Algoritmos de construção de Árvores de Decisão buscam _minimizar a informação necessária p/ classificar os dados nas partições da Árvore_.

## 6.1.5 - Algoritmos de Classificação

### 1 - Random Forest

- > Def. — É um algoritmo de Classificação baseado em Árvores de Decisão.

  E.g. [ Vide SLIDE ]

### 2 - k–Nearest Neighbors (k-NN)

- > Algoritmo **não paramétrico**.

- > Def. — É um algoritmo que _classifica novos dados com base em uma medida de similaridade entre seus **"vizinhos" mais próximos**_, ou seja, aqueles que têm características semelhantes às suas.

- > Nesse método, utiliza-se a _distância (usualmente a Euclidiana) entre uma nova observação e as demais observações de um Conjunto de Treinamento_ p/ classificá-la de acordo com a observação mais próxima.

- > O "k" determina o _número de vizinhos que serão utilizados p/ a classificação_.

  E.g. [ Vide SLIDES ]

- > O k-NN tbm é considerado um **Método de Imputação de Dados**.

### 3 - Naive Bayes

- > ?? Algoritmo paramétrico ??

- > Def. — Família de algoritmos de Aprendizado Supervisionado baseados em probabilidade.

- **Teorema de Bayes:**

      P(A/B) = P(B/A) * P(A) / P(B)

  onde...

  - `P(A/B)` é a probde. de A ser Verd. dado que B é Verd..
  - `P(B/A)` é a probde. de B ser Verd. dado que A é Verd..
  - `P(A)` é a probde. de A ser Verd..
  - `P(B)` é a probde. de B ser Verd..

  E.g. [ Vide SLIDE ]

#### Características

- **Suposições ingênuas**

  - > Todas as características do Conjunto de Dados _não dependem umas das outras_.
  - > Sendo assim, cada evento _contribui igualmente_ p/ classificar o resultado.

- **Rápido**

  > Previsão em tempo real.

- Requer uma **quantidade reduzida de amostras**, se comparado a outros Algoritmos.

- Famoso como Classificador de **spam, sentimentos e recomendações**.

#### Funcionamento

1. > Prevê uma Tabela de Probabilidades Condicionais,

2. > E depois _calcula a saída com base nessa tabela_.

### 4 - Regressão Logística

- > Algoritmo **paramétrico**.

- > _Apesar do nome_, a Regressão Logística é **usada em problemas de Classificação**!

- > Def. — É uma técnica estatística que tem como objetivo _produzir, a partir de um conjunto de observações, um modelo que permita a predição de valores tomados por uma variável categórica_, frequentemente binária, _a partir de uma série de variáveis explicativas contínuas e/ou binárias_.

  [ EGVS ]

#### Função Logística

               1
    σ(x) = ——————————
           1 + e^(-x)

- Quanto maior for `x`, mais inclinada fica a função logística.

  [ EGVS ]

#### Características

- Aceita **variáveis numéricas ou categórias**.

- A variável `y` tem **distribuição binomial e binária** na maioria das vezes.

- Usa o **método da Máxima Verossimilhança**.

  > P/ fazer a _previsão do parâmetro da Função Logística **ou da Função Linear**_.

- Precisa apenas de um **pequeno número de suposições** p/ realizar as predições.

- Retorna a **probabilidade**.

#### Tipos

- Regressão Logística **Binominal**

  - > Os objetos são classificados em _2 grupos ou categorias_.

- Regressão Logística **Ordinal**

  - > Trabalha com o conceito de Categorias Ordenadas.

  - > Os objetos são classificados em 3 ou mais classes que _possuem uma ordem já determinada_.

- Regressão Logística **Multinomial**

  - > Os objetos são classificados em 3 ou mais categorias que _não possuem ordem entre si_.

### 5 - SVM (Support Vector Machines)

- > Def. — O conceito por trás das Máquinas de Vetor de Suporte (SVM) é a **maximização da margem**, ou seja, _maximizar a distância da margem dos dados de treinamento_.

  [ EGVS ]

#### Hiperplano Ótimo

- Def.:

  > Distância da margem p/ o exemplo da classe _positiva_ <br> = <br>
  > Distância da margem p/ o exemplo da classe _negativa_.

- > O Hiperplano no R2 é uma _linha_, e o Hiperplano no R3 é um _plano_.

  [ EGVS ]

#### Vetores de Suporte

- > Def. — São os exemplos da Base de Treinamento _mais próximos do Hiperplano_.

- > O Hiperplano é _definido unicamente pelos Vetores de Suporte_, os quais são encontrados durante o Treinamento.

#### Problemática e Solução

- > A grande maioria dos problemas reais _**não** são linearmente separáveis_.

- > A pergunta então é: "Como resolver problemas que não são linearmente separáveis com um classificador linear?"

- Solução:

  > 1. Projetar os dados em um espaço onde os dados são linearmente separáveis.
  >
  > 2. Projetar os dados em outra dimensão usando uma Função de Kernel (Kernel Trick).
  >
  > 3. Encontrar um Hiperplano que separe os dados nesse espaço.

#### Histórico

- Linear (1963)

- Kernel Trick (1992)

- Soft Margin (1995)

## 6.1.6 - Redes Neurais Artificiais (RNA)

- > Modelo baseado no neurônio.

  [ Vide SLIDE p/ representação de um neurônio. ]

- > Def. — Consiste em uma **estrutura conexionista**—Paradigma Conexionista—na qual o processamento é _distribuído por um grande número de pequenas unidades densamente interligadas_.

- > A habilidade das RNAs em realizar **mapeamentos não-lineares** entre entradas e saídas as tem tornado prósperas no _reconhecimento de padrões_ e na _modelagem de sistemas complexos_.

### Exemplos de uso de RNA

> - Avaliação de imagens captadas por satélite.
>
> - Classificação de padrões de escrita e fala.
>
> - Reconhecimento de faces com visão computacional.
>
> - Sistemas de controle e previsão financeira.
>
> - Identificação de anomalias e patologias na área médica com base em imagens.
>
> - Controle automatizado de equipamentos eletrônicos.

### Perceptron

- > Resolvem problemas lineares.

- Funcionamento:

  1. > Sinais são apresentados à entrada.

  2. > Cada sinal é _multiplicado por um número_, ou peso, que indica a sua **influência** na saída da unidade.

  3. > É feita a _soma ponderada dos sinais_ que produzem um **Nível de Atividade**.

  4. > Se este Nível de Atividade exceder um certo Limite (Threshold), a unidade produz uma determinada **resposta de saída**.

  [ Vide SLIDE p/ esquema de um Perceptron. ]

### RNA 1 - Redes Multilayer Perceptron (MLP)

- Organizada em diversas Camadas:

  - Uma Camada de **Entrada**

    > Formada pelos Neurônios que estão conectados às entradas da rede.

  - Uma ou mais Camadas **Intermediárias** (ou Escondidas)

    > Compostas de Neurônios cujas entradas e saídas estão conectadas somente a outros Neurônios.

  - Uma Camada de **Saída**

    > Contendo os Neurônios que apresentam as saídas da Rede Neural ao ambiente externo.

  [ EGVS ]

#### Feed-Forward

> Na maioria das aplicações, usa-se uma Rede Feed-Forward com...

- > **uma única Camada Escondida**,

- > **Função de Ativação Sigmóide** dos Neurônios da Camada Escondida, e

- > **Função de Ativação Linear** nos Neurônios da Camada de Saída.

#### Treinamento por Algoritmo _Backpropagation_

- > Def. — Correção dos Pesos pela _diferença entre o valor Obtido e o Esperado_.

  [ VS p/ esquema ]

- A ideia principal deste algoritmo de aprendizado é que _os erros das unidades da Camada Escondida sejam determinados retro-propagando os erros da Camada de Saída_.

#### Funcionamento

1. **Passo p/ Frente** (Forward Pass)

   1. > As entradas são passadas através da rede e obtêm-se as Previsões de Saída.

   - T.c.c. Fase de Propagação.

2. **Passo p/ Trás** (Backward Pass)

   1. > Nessa fase calcula-se o gradiente da função de perda na Camada Final (i.e. Camada de Previsão) da rede,

   2. > e usa-se esse gradiente p/ aplicar recursivamente a Regra da Cadeia (Chain Rule) p/ atualizar os pesos da rede.

   - T.c.c. Fase de Atualização ou Retro-Propagação

#### Método do Gradiente Descendente

- > Ajusta os parâmetros das Redes p/ melhor adaptar um Conjunto de Treinamento de pares entrada-saída.

#### Taxa de Aprendizado

- > Indica _a qual ritmo_ os pesos são _atualizados_.

- > Isso pode ser _fixado ou alterado de modo adaptativo_.

- > O método atual mais popular é o **Adam**, que é um método que _adapta a Taxa de Aprendizado_.

#### Quando deve-se parar o treinamento (i.e. parar de atualizar os pesos)?

- Uma escolha óbvia seria:

  - > Continuar o treinamento até que o erro seja menor que um valor pré-estabelecido.
  - > Porémmm, isto _implica em Sobreajuste (Overfitting)_ !!!

  [ VS p/ gráfico ]

- Mínimos Locais:

  - > O treinamento é um _processo de otimização por gradiente em uma superfície_.

  - > Nesta superfície existem "vales" com menor profundidade que outros. _Estes são os Mínimos Locais e não representam a melhor solução_.

  - > O algoritmo Backpropagation pode _ficar preso em um Mínimo Local_, não permitindo que se encontre a _melhor solução_ do problema mapeado pela rede.

### RNA 2 - Rede Recorrente ou Retroalimentada (RNN)

- **Descrição Geral**:
  - As RNNs são redes projetadas para processar sequências de dados, onde a ordem e o contexto importam.
  - Utilizam suas conexões cíclicas para manter informações de estados anteriores, funcionando como uma memória de curto prazo.
- **Aplicações**:
  - Análise de linguagem natural: processamento de texto para tradução automática ou geração de texto.
  - Previsão de séries temporais: como previsão do tempo ou preços de ações.
- **Vantagens**:
  - Capacidade de modelar sequências de qualquer comprimento.
  - Uso da informação contextual para realizar previsões mais precisas.

### RNA 3 - Markov Chain

- Baixa probabilidade em concursos.

#### Descrição Geral

- Modelo estatístico que descreve um sistema que transita entre estados, com transições dependentes apenas do estado atual.

#### Funcionamento

- Estado atual influencia diretamente o próximo estado, sem memória de estados anteriores.
- Utilizado para modelar processos de decisão simples e sequências onde a ordem imediata importa.

#### Aplicações

- Biologia evolutiva para estudar mutações genéticas.
- Modelagem de processos de decisão em sistemas econômicos ou jogos.

### RNA 4 - Rede Neural Convolucional (CNN)

- Convolutional Neural Network (CNN) ou Deep Convolutional Network (DCN)

- **Alta** probabilidade em concursos.

#### Descrição Geral

- Especializadas em processar dados com uma grade topológica, como imagens.

#### Componentes Principais

- **Camadas de Convolução**: Utilizam filtros para capturar características locais dos dados.
- **Camadas de Pooling**: Reduzem a dimensionalidade do mapa de características, mantendo os traços mais significativos.

#### Processo de Funcionamento

- As características são extraídas através das camadas de convolução.
- Redução de dimensionalidade via pooling para simplificar a informação.
- Classificação ou outras tarefas são realizadas nas camadas finais.

#### Aplicações

- Reconhecimento de imagens e vídeos.
- Análise de documentos e reconhecimento de voz.

### RNA 5 - Mapas Auto Organizáveis (SOM)

- Self-Organizable Maps (SOM)

- Resolve problemas de **Agrupamento**.

#### Descrição Geral

- Tipo de rede neural que utiliza aprendizado não supervisionado para organizar dados em um espaço de menor dimensão.

#### Funcionamento

- Neurônios competem para representar os dados, com um vencedor para cada instância de dados.
- A localização dos neurônios no mapa reflete as características intrínsecas dos dados, agrupando entradas similares juntas.

#### Aplicações

- Visualização de dados multidimensionais.
- Agrupamento de dados para descoberta de padrões semelhantes.

#### Vantagens

- Facilita a visualização e interpretação de grandes conjuntos de dados.
- Ajuda a identificar clusters e padrões ocultos nos dados.

## 6.1.7 - Funções de Ativação

### 1 - Função Limiar

- **Descrição**: Uma função limiar gera uma saída binária baseada em um limiar. Se a entrada for maior que o limiar, a saída será um; caso contrário, será zero.
- **Fórmula**:
  $$
  f(x) = \begin{cases}
  1 & \text{se } x \geq 0 \\
  0 & \text{se } x < 0
  \end{cases}
  $$
- **Gráfico**:
  - [ VS ]
  - Representação gráfica de uma função de limiar.

### 2 - Função Linear

- **Descrição**: Uma função linear aplica um fator de escala à entrada, ou seja, a saída é proporcional à entrada.
- **Fórmula**:
  $$ f(x) = ax $$
- **Gráfico**:
  - [ VS ]
  - Representação gráfica de uma função linear com diferentes inclinações.

### 3 - Função Logística (Sigmoide)

> **Mais cobrada** em concursos. Recomenda-se **decorar a fórmula**.

- **Descrição**: Produz uma saída entre 0 e 1, útil para modelar probabilidades.
- **Fórmula**:
  $$ f(x) = \frac{1}{1 + e^{-x}} $$
- **Gráfico**:
  - [ VS ]
  - Curva em forma de 'S' que suaviza a transição entre os estados 0 e 1.

### 4 - Função Softmax

- **Descrição**: Generaliza a sigmóide para múltiplas classes, distribuindo probabilidades entre várias saídas.
- **Fórmula**:
  $$
  \phi_i = \frac{e^{z_i}}{\sum_{j \in \text{group}} e^{z_j}}
  $$
- **Gráfico**:
  - [ VS ]
  - Mostra a probabilidade atribuída a cada classe com base nas entradas.

### 5 - Função Maxout

- **Descrição**: Retorna o valor máximo entre as entradas, proporcionando uma capacidade de modelagem robusta.
- **Fórmula**: Não há uma fórmula específica além de selecionar o máximo.
- **Gráfico**:
  - [ VS ]
  - Demonstração de como diferentes entradas são mapeadas para o máximo.

### 6 - Funçao Gaussiana

- **Descrição**: Utilizada em Redes de Função de Base Radial (RBF), onde a saída decresce ou cresce exponencialmente a partir do centro de um ponto de dados.
- **Fórmula**:
  $$
  \phi_j(x) = \exp\left(-\frac{(x - c_j)^2}{2\sigma_j^2}\right)
  $$
- **Gráfico**:
  - [ VS ]
  - Forma de sino típica da distribuição gaussiana.

### Outras Funções de Ativação

#### Função Tangente Hiperbólica (Tanh)

- **Descrição**:
  - Similar à Gaussiana/Sigmoide, mapeia a entrada para saídas no intervalo [-1, 1], oferecendo uma transição suave.
  - Diferentemente da Gaussiana, a Tanh centraliza a ativação em zero.
- **Fórmula**:
  $$ \text{Tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
- **Gráfico**:
  - [ VS ]
  - Curva em forma de 'S' que estica de \(-1\) a \(1\), centralizando a saída.

#### ReLU

- **Descrição**: Retorna zero para entradas negativas e a própria entrada para valores positivos.
- **Fórmula**:
  $$ f(x) = \max(0, x) $$

#### Leaky ReLU

- **Descrição**: Permite um pequeno gradiente quando a entrada é negativa.
- **Fórmula**:
  $$
  g(z) = \max(\epsilon z, z) \quad \text{com } \epsilon \ll 1
  $$

#### Comparação entre ReLU e Leaky ReLU

- **Gráficos**:
  - [ VS ]

## 6.1.8 - Regressão

&emsp;[ FFVS ]

### Regressão Linear

&emsp;[ FFVS ]

#### Resíduo e Erro

&emsp;[ FFVS ]

#### Regressão Linear _Simples_ vs _Múltipla_

&emsp;[ FFVS ]

#### Correlação _Positiva_ vs _Negativa_

&emsp;[ FFVS ]

## 6.1.9 - Algoritmos de Agrupamento (Clusterização)

&emsp;[ FFVS ]

### Subjetividade dos grupos

&emsp;[ FFVS ]

### Distâncias _Intra_-Cluster vs _Inter_-Cluster

&emsp;[ FFVS ]

### Etapas do processo

&emsp;[ FFVS ]

### Seleção de atributos e Medida de proximidade

&emsp;[ FFVS ]

1. Seleção de atributos
2. Medida de proximidade
3. Critério de agrupamento
4. Algoritmo de agrupamento
5. Verificação dos resultados
6. Interpretação dos resultados

### Medidas de Dissimilaridade e de Similaridade

> **Muito baixa** probabilidade de cair.

&emsp;[ FFVS ]

### Tipos de algoritmo de Clusterização

&emsp;[ FFVS ]

- **Hierárquicos**
- **Particionais**
- Baseados em Densidade
- Baseados em Grade
- Baseados em Modelos
- Baseados em Redes Neurais
- Baseados em Lógica Fuzzy
- Baseados em Kernel
- Baseados em Grafos
- Baseados em Computação Evolucionária

#### Subtipos de Hierárquicos e de Particionais

- Hierárquicos

  - Single Link
  - Complete Link

- **Particionais**
  - **Métrica Euclidiana**
    - **k-means**
  - Teoria de Grafos
  - Mistura de Densidades
    - Expectation Maximization

#### Algoritmos Hierárquicos

&emsp;[ FFVS ]

#### Algoritmos de Particionamento

&emsp;[ FFVS ]

- Subtipos:
  - Aglomerativos (bottom-up)
  - Divisivos (top-down)

#### Outros tipos de algoritmo

&emsp;[ FFVS ]

### 1 - K-Means

&emsp;[ FFVS 6.1.9-2 ]

---

Aula 6.1.10-1

## 6.1.10 - Deep Learning

### Introdução

&emsp;[ FFVSs ]

---

Aula 6.1.10-2

### Arquiteturas de Redes Neurais

- Redes Neurais **Feed-Forward**
  - Uma camada de entrada, várias camadas intermediáras, uma camada de saída.
- Redes **Recorrentes**
  - O fluxo pode retornar p/ a camada ou neurônio anterior.
- Redes **Conectadas Simetricamente**
  - Conexões simétricas entre os neurônios.

### Categorias de Redes Neurais

- Redes Multilayer Perceptron
- Redes Neurais Convolucionais
- Redes Neurais Recorrentes
  - Long Short-Term Memory (LSTM)
  - Máquinas de Boltzmann
    - Deep Belief Network
      - Deep Auto-Encoders
- Redes de Hopfield
- Generative Adversarial Network
- Deep Neural Network Capsules

### Redes Neurais Convolucionais

&emsp;[ FFVS ]

---

Aula 6.1.10-3

### Redes Neurais Recorrentes

&emsp;[ FFVS ]

- **Long Short-Term Memory (LSTM):**

  &emsp;[ FFVS ]

- **Máquinas de Boltzmann:**

  &emsp;[ FFVS ]

  - **Deep Belief Network (DBN):**

    &emsp;[ FFVS ]

    - **Deep Auto-Encoders:**

      &emsp;[ FFVS ]

### Redes de Hopfield

&emsp;[ FFVSs ]

### Generative Adversarial Network

&emsp;[ FFVS ]

### Deep Neural Network Capsules

&emsp;[ FFVS ]
